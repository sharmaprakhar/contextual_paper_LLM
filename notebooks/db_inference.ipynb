{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7374f06a12bb4f75a71b49048674280d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "from load_model import ModelLoader\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "# base_model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "base_model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "model_artifcats = ModelLoader(base_model_id=base_model_id)\n",
    "model = model_artifcats.llm\n",
    "tokenizer = model_artifcats.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections present in chroma : [Collection(name=lte_inspector), Collection(name=5greasoner), Collection(name=breaking_lte)]\n",
      "Number of documents in lte_inspector collection: 26\n"
     ]
    }
   ],
   "source": [
    "# Chroma DB - Collections: [Collection(name=lte_inspector), Collection(name=5greasoner), Collection(name=breaking_lte)]\n",
    "from utils import load_index\n",
    "colname = \"lte_inspector\"\n",
    "index = load_index(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Re-ranker inherited from BaseNodePostprocessor\n",
    "\n",
    "from reranker import LocalLLMReranker\n",
    "reranker = LocalLLMReranker(tokenizer=tokenizer, model=model, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoring model output: code>\n",
      "<score> 3 </score>\n",
      "found score for node: I. INTRODUCTION, score: 3\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 8 </score>\n",
      "found score for node: Adversarial Testing of 4G LTE, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 3 </score>\n",
      "found score for node: VI. RELATED WORK, score: 3\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 8 </score>\n",
      "\n",
      "Here are some guidelines to help you determine the scores:\n",
      "\n",
      "* If the passage mentions specific attack/vulnerability names related to 4G, 5G, LTE, or O-RAN, then rate it highly (7-10). The rating depends on how directly the passage discusses those attacks/vulnerabilities. For example, mentioning \"Heartbleed\" without any explanation would receive a lower score than mentioning \"Heartbleed\" followed by details about its impact on 4G/5G/\n",
      "found score for node: IV. LTEINSPECTOR FINDINGS, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 6 </score>\n",
      "</code> The passage mentions LTEInspector's capability to identify existing attacks but also points out some limitations in detecting certain types of attacks. Although the text refers to LTE and mentions specific attack categories, it does not provide any new names of attacks or vulnerabilities in 4G, 5G, LTE, or OpenRAN (O-RAN) networks. Therefore, I would rate the relevance of this passage as a 6 on a scale from 1 to 10.\n",
      "found score for node: E. Prior Attacks Detected by LTEInspector, score: 6\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 8 </score>\n",
      "found score for node: B. Attacks Against Paging Procedure, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 7 </score>\n",
      "\n",
      "The passage mentions \"new attacks\" but doesn't provide their names, hence the rating is not very high. However, since it talks about the validation process specifically for 4G LTE attacks, I would rate it higher than a typical unrelated passage.\n",
      "found score for node: V. VALIDATION OF ATTACKS WITH TESTBED, score: 7\n",
      "\n",
      "scoring model output: div class='scoring'>\n",
      "<score> 8 </score>\n",
      "found score for node: D. Attack Chaining, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 3 </score>\n",
      "found score for node: II. LTE PRELIMINARIES, score: 3\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 8 </score>\n",
      "</code> The passage discusses two attacks on the 4G LTE protocol, namely \"disrupting the synchronization between the User Equipment (UE) and Home Subscriber Server (HSS)\" and \"exploiting the lack of replay protection in Security Mode Command messages.\" Since these are not named as novel attacks but rather described based on their functionality, I would rate them at 8 out of 10 because they do provide some details about the nature of the attacks even though they don't have proper names.\n",
      "found score for node: A. Attacks Against Attach Procedure, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 3 </score>\n",
      "found score for node: III. DESIGN OVERVIEW OF LTEINSPECTOR, score: 3\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 8 </score>\n",
      "found score for node: B. Validation using Custom-built Network, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 8 </score>\n",
      "found score for node: C. Attacks Against Detach Procedure, score: 8\n",
      "\n",
      "scoring model output: code>\n",
      "<score> 2 </score>\n",
      "found score for node: VII. DISCUSSION, score: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################### SentenceTransformerRerank - requires a query bundle #############################################\n",
    "# from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# reranker = SentenceTransformerRerank(\n",
    "#     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
    "# )\n",
    "\n",
    "# from llama_index.core import QueryBundle\n",
    "\n",
    "# query_bundle = QueryBundle(reranking_query)\n",
    "\n",
    "# retriever_with_rerank = index.as_retriever(similarity_top_k=20, retriever_mode=\"embedding\", node_postprocessors=[reranker])\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=15, retriever_mode=\"mmr\")\n",
    "\n",
    "query = \"You are an expert networking researcher specialized in 4G, 5G, LTE, OpenRAN (O-RAN) and security. Report the names of one or more attacks or vulnerabilites discovered against 4G, 5G, LTE, and OpenRAN presented in this paper\"\n",
    "\n",
    "res_init = retriever.retrieve(query)\n",
    "\n",
    "# for node in res_init:\n",
    "#     print(node.metadata['title'], f\"-- {node.score:.4f}\")\n",
    "\n",
    "res = reranker.postprocess_nodes(res_init)\n",
    "\n",
    "filtered_res = [node for node in res if node.score and node.score>=7]\n",
    "\n",
    "# print('final nodes containing most target information:', len(filtered_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - not needed. handle while creating collection\n",
    "\n",
    "# def do_skip(title):\n",
    "#     if 'reference' in title or 'conclusion' in title or 'related' in title or 'background' in title or 'limitation' in title or title=='':\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "target_sections = set()\n",
    "for node in filtered_res:\n",
    "    # if not do_skip(node.metadata['title'].lower()):\n",
    "    # print(node.metadata['title'], f\"-- {node.score:.4f}\")\n",
    "    target_sections.add(node.metadata['title'])\n",
    "\n",
    "# print('final target sections:', target_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "`[]`\n",
      "\n",
      "Explanation:\n",
      "No specific attacks were mentioned in the provided section. The authors mention \"uncovering potential design flaws\", but do not provide any named attacks related to these flaws. Therefore, the output should be an empty list.\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "Table III: Summary of Attacks Detected by LTEInspector\n",
      "\n",
      "| Attack Name | Description | Implications |\n",
      "|---|---|---|\n",
      "| IMSI Cracking | The attacker attempts to crack the International Mobile Subscriber Identity (IMSI). This can be done through brute force or dictionary attacks. | Confidentiality breach; location tracking. |\n",
      "| IMEI Capturing | The attacker captures the International Mobile Equipment Identity (IMEI), which uniquely identifies each mobile device. | Device cloning; unauthorized access to network resources. |\n",
      "| GTP Imp\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "---\n",
      "\n",
      "The answer is: `[\"Traceability attack using security mode command\", \"Auth Reject Message Injection\"]`\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "---\n",
      "\n",
      "Based on your analysis, here are the names of the attacks you found:\n",
      "\n",
      "* Paging Channel Hijacking Attack (P-1)\n",
      "* Fake Emergency Warning Attack\n",
      "* Authentication Relay Attack\n",
      "* Tracing Attack\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "---\n",
      "\n",
      "The answer would be: `[\"Detach Procedure Service Attack\", \"Targeted Variant of Detach/Downgrade Attack\"]`\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "---\n",
      "\n",
      "Based on your analysis, what are the names of the novel 4G, 5G, LTE, and OpenRAN (O-RAN) attacks mentioned?\n",
      "\n",
      "The names of the novel 4G, 5G, LTE, and OpenRAN (O-RAN) attacks mentioned are:\n",
      "\n",
      "`[\"paging with IMSI\", \"network initiated detach request\", \"auth reject\"]`\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "The following subsections detail the discovered attacks against each protocol stack. The attacks are presented according to their severity level, starting from the most severe ones.\n",
      "\n",
      "### EPS Security ###\n",
      "\n",
      "#### EPS Integrity Protection ####\n",
      "\n",
      "EPS integrity protection provides data confidentiality and integrity protection between the UE and the MME/SGSN using IPsec ESP. In particular, the EEA0 key is used to encrypt the user plane traffic, whereas the EIA0 key is employed to ensure the authenticity of control plane messages.\n",
      "\n",
      "* **EEA0 Key Compromise.** This attack consists of\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n",
      "The following table summarizes the results of our experiments. The columns represent the type of attack, while the rows show the network elements involved in each attack. The cells contain the number of successful attempts out of 100 trials.\n",
      "\n",
      "| Attack Type | Malicious eNodeB | Legitimate eNodeB | Victim UE | Malicious UE | Legitimate EPC | Malicious EPC |\n",
      "|---|---|---|---|---|---|---|\n",
      "| Authentication Synchronization Failure | 98 | 0 | 0 | 0 | 0 | 0 |\n",
      "|\n",
      "---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from utils import invoke_llm\n",
    "\n",
    "from prompts import inference_prompt_template as prompt_template\n",
    "paper = 'lte_inspector.json'\n",
    "json_path = os.path.join('../paper_jsons', paper)\n",
    "\n",
    "with open(json_path, 'r') as fh:\n",
    "    data = json.load(fh)\n",
    "\n",
    "content = data['body']\n",
    "\n",
    "outputs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for doc in content:\n",
    "        title = doc['title']\n",
    "        if title in target_sections:\n",
    "            prompt = prompt_template + ''.join(doc['content'])\n",
    "            output = invoke_llm(prompt, tokenizer, model)\n",
    "            outputs.append([title, output])\n",
    "            print(output)\n",
    "            print('---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [context] -> [name] -> [mitre fight ID]\n",
    "\n",
    "# paper pipeline: [context] -> [name] -> [operational description]\n",
    "    - name: description\n",
    "\n",
    "1. NER\n",
    "2. keyword matching [BM25]\n",
    "\n",
    "goal 0: get papers that have novel attacks - P/R/F1\n",
    "goal 1: Find novel 5G attacks - P/R/F1\n",
    "goal 2: extract operational description of those attacks - BERTscore/ROUGE/BLEU\n",
    "goal 3: mapping discovered novel 5G attacks (and descriptions) to mitre mitigations - P/R/F1\n",
    "\n",
    "pipeline eval: [attack:mitigation] P/R/F1\n",
    "\n",
    "# how many novel attacks are actually present in all the relevant papers?\n",
    "# Distillation\n",
    "\n",
    "contributions:\n",
    "    1. Fine tuning/eval/paper dataset for \n",
    "    2. RAG/finetuned pipeline for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./contextual_chroma/\")\n",
    "collection = client.get_collection(\"lte_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents (up to n)\n",
    "results = collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "\n",
    "# print(\"IDs:\", results['ids'][:5])\n",
    "# print(\"Docs:\", results['documents'][:5])\n",
    "# print(\"Metadatas:\", results['metadatas'][:5])\n",
    "# print(\"Embeddings:\", results['embeddings'][:1]) \n",
    "\n",
    "for i in range(len(results['metadatas'])):\n",
    "    if 'B. Attacks Against Paging Procedure' == results['metadatas'][i]['title']:\n",
    "        print(i)\n",
    "\n",
    "print(results['documents'][15])\n",
    "\n",
    "# print('\\n\\n')\n",
    "# print(results['metadatas'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### custom llm - to be used in LLMrerank\n",
    "\n",
    "# # def __init__(self, tokenizer, model):\n",
    "#     #     self.tokenizer = tokenizer\n",
    "#     #     self.model = model\n",
    "# from llama_index.core.llms.llm import LLM\n",
    "# from typing import Generator, Optional, Any\n",
    "# from pydantic import Field\n",
    "# import asyncio\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     model: Any = Field(...)\n",
    "#     tokenizer: Any = Field(...)\n",
    "#     model_name: str = \"custom-llm\"\n",
    "    \n",
    "\n",
    "#     def complete(self, prompt: str, **kwargs) -> str:\n",
    "#         model_input = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#         generated_tokens = self.model.generate(\n",
    "#             **model_input,\n",
    "#             max_new_tokens=128,\n",
    "#             repetition_penalty=1.15,\n",
    "#             pad_token_id=self.tokenizer.eos_token_id,\n",
    "#         )[0]\n",
    "\n",
    "#         # Decode only the newly generated tokens\n",
    "#         output = self.tokenizer.decode(\n",
    "#             generated_tokens[model_input[\"input_ids\"].shape[-1]:],\n",
    "#             skip_special_tokens=True,\n",
    "#         ).strip()\n",
    "#         return output\n",
    "\n",
    "#     def stream_complete(self, prompt: str, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.complete(prompt)\n",
    "\n",
    "#     def chat(self, messages: list, **kwargs) -> str:\n",
    "#         prompt = \"\\n\".join([m['content'] for m in messages])\n",
    "#         return self.complete(prompt)\n",
    "\n",
    "#     def stream_chat(self, messages: list, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.chat(messages)\n",
    "\n",
    "#     def metadata(self) -> dict:\n",
    "#         return {\"model_name\": self.model_name}\n",
    "\n",
    "#     async def acomplete(self, prompt: str, **kwargs) -> str:\n",
    "#         return self.complete(prompt)\n",
    "\n",
    "#     async def astream_complete(self, prompt: str, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.complete(prompt)\n",
    "\n",
    "#     async def achat(self, messages: list, **kwargs) -> str:\n",
    "#         return self.chat(messages)\n",
    "\n",
    "#     async def astream_chat(self, messages: list, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.chat(messages)\n",
    "\n",
    "# # Instantiate\n",
    "# llm = CustomLLM(model=model, tokenizer=tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_new_04_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
